# 命令行工具的使用
## 命令行工具简述
目前scrapy的版本为0.01版本，通过scrapy 子命令 参数 这种方式使用。移除了scrapy deploy命令， 你可以访问如下网址了解详情[Deploying your project](https://scrapyd.readthedocs.io/en/latest/deploy.html)。
## 配置设置
|   |  Unix | Window|
|---|---|---|
|系统级|/etc/scrapy.cfg|c:\scrapy\scrapy.cfg|
|用户级|\~/.config/scrapy.cfg ($XDG_CONFIG_HOME)，\~/.scrapy.cfg ($HOME)| 和Unix一样|
|工程级|scrapy.cfg|和Unix一样|
用户级配置是会覆盖系统级配置， 工程级配置是会覆盖用户级配置。

## 使用爬虫工具
```cmd
C:\Users\Administrator>scrapy
Scrapy 1.3.3 - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  commands
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use "scrapy <command> -h" to see more info about a command

```
子命令简单说明
* bench: 取取URL使用Scrapy下载
* genspider：产生新的蜘蛛使用预先定义的模板
* unspider： 运行一个独立的蜘蛛（不创建项目）
* settings： 获取设置值
* shell： 互动式爬虫控制台
* startproject： 创建新项目
* version： 打印scrapy版本
* view： 在浏览器视图中打开网址

全局可用命令（任意目录可以执行）
* startproject
* genspider
* settings
* runspider
* shell
* fetch
* view
* version

工程级命令（仅仅在工程目录内可用命令）
* crawl
* check
* list
* edit
* parse
* bench
### startproject
 * 用法:scrapy startproject <project_name> [project_dir]

* 需要工程：否

在`project_dir`目录创建一个项目， 项目名字为`project_name`，如果`project_dir`没有指定的话，`project_dir`会和`project_name`一样。其中工程目录支持相对和绝对路径的。

我们在demo目录测试下看看效果。
```cmd
e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo>scrapy startproject test01 dir01
New Scrapy project 'test01', using template directory 'C:\\Program Files\\Anaconda3\\lib\\site-packages\\scrapy\\templates\\project', created in:
    e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\dir01

You can start your first spider with:
    cd dir01
    scrapy genspider example example.com

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo>scrapy startproject test02
New Scrapy project 'test02', using template directory 'C:\\Program Files\\Anaconda3\\lib\\site-packages\\scrapy\\templates\\project', created in:
    e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02

You can start your first spider with:
    cd test02
    scrapy genspider example example.com
e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo>tree
卷 新加卷 的文件夹 PATH 列表
卷序列号为 000000C7 D20B:7155
E:.
├─dir01
│  └─test01
│      ├─spiders
│      │  └─__pycache__
│      └─__pycache__
├─quotesbot-master
│  └─quotesbot
│      └─spiders
├─test02
│  └─test02
│      ├─spiders
│      │  └─__pycache__
│      └─__pycache__
└─tutorial
    └─tutorial
        ├─spiders
        │  └─__pycache__
        └─__pycache__
```
我们发现，我们没有指定工程目录的话，工程目录默认和工程名字一样了，如果我们指定了，就使用我们指定的工程目录。

### genspider

* 用法: scrapy genspider [-t template] <name> <domain>
* 需要工程: 否
name指定爬虫的名字，domain会在爬虫文件中生成allowed_domains和start_urls爬虫属性。

使用样例
```
e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed
```
我们可以看到一共提供了4个模板。
我们在样例文件中分别创建者4个爬虫类，看看到底是啥。
```
C:\Users\Administrator>cd e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02

C:\Users\Administrator>e:

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>dir
 驱动器 E 中的卷是 新加卷
 卷的序列号是 D20B-7155

 e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02 的目录

2017/10/19  11:51    <DIR>          .
2017/10/19  11:51    <DIR>          ..
2017/10/19  11:51               256 scrapy.cfg
2017/10/19  11:51    <DIR>          test02
               1 个文件            256 字节
               3 个目录 172,873,003,008 可用字节

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>scrapy genspider -t basic test_base www.baidu.com
Created spider 'test_base' using template 'basic' in module:
  test02.spiders.test_base

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>scrapy genspider -t crawl test_crawl www.baidu.com
Created spider 'test_crawl' using template 'crawl' in module:
  test02.spiders.test_crawl

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>scrapy genspider -t csvfeed test_csvfeed www.baidu.com
Created spider 'test_csvfeed' using template 'csvfeed' in module:
  test02.spiders.test_csvfeed

e:\ZhaojiediProject\github\My_Study_Scrapy\官方文档学习\demo\test02>scrapy genspider -t xmlfeed test_xmlfeed www.baidu.com
Created spider 'test_xmlfeed' using template 'xmlfeed' in module:
  test02.spiders.test_xmlfeed
```
执行类似如上的操作。我们可以看到在工程目录下生成了几个文件分别看看
base模板结果
``` python
# -*- coding: utf-8 -*-
import scrapy


class TestBaseSpider(scrapy.Spider):
    name = "test_base"
    allowed_domains = ["www.baidu.com"]
    start_urls = ['http://www.baidu.com/']

    def parse(self, response):
        pass

```
crawl模板结果
```python
# -*- coding: utf-8 -*-
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class TestCrawlSpider(CrawlSpider):
    name = 'test_crawl'
    allowed_domains = ['www.baidu.com']
    start_urls = ['http://www.baidu.com/']

    rules = (
        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        i = {}
        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()
        #i['name'] = response.xpath('//div[@id="name"]').extract()
        #i['description'] = response.xpath('//div[@id="description"]').extract()
        return i

```
csvfeed模板结果
```python
# -*- coding: utf-8 -*-
from scrapy.spiders import CSVFeedSpider


class TestCsvfeedSpider(CSVFeedSpider):
    name = 'test_csvfeed'
    allowed_domains = ['www.baidu.com']
    start_urls = ['http://www.baidu.com/feed.csv']
    # headers = ['id', 'name', 'description', 'image_link']
    # delimiter = '\t'

    # Do any adaptations you need here
    #def adapt_response(self, response):
    #    return response

    def parse_row(self, response, row):
        i = {}
        #i['url'] = row['url']
        #i['name'] = row['name']
        #i['description'] = row['description']
        return i

```
xmlfeed模板结果
```python
# -*- coding: utf-8 -*-
from scrapy.spiders import XMLFeedSpider


class TestXmlfeedSpider(XMLFeedSpider):
    name = 'test_xmlfeed'
    allowed_domains = ['www.baidu.com']
    start_urls = ['http://www.baidu.com/feed.xml']
    iterator = 'iternodes' # you can change this; see the docs
    itertag = 'item' # change it accordingly

    def parse_node(self, response, selector):
        i = {}
        #i['url'] = selector.select('url').extract()
        #i['name'] = selector.select('name').extract()
        #i['description'] = selector.select('description').extract()
        return i

```
我们这里可以看出来这4个模板的不同的，csvfeed和xmlfeed主要用于csv和xml格式的解析好些， 使用parse_row去一行一行解析csv的行数据， 使用parse_node方法解析xml的每个节点。crawl设定有rule，方便匹配指定格式的url去指定解析方法， base适合入门网址使用。
